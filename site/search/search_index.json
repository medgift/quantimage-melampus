{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Melampus documenation Welcome! This the documentation for Melampus 0.0.3, the machine learning infrastracture package for SPHN-Imagine project Melampus consists of three modules: Preprocessor Feature Selector Classifier Installation For production mode: pip install -i https://test.pypi.org/simple/ quantimage-melampus Prerequisites commands for developers : source venv/bin/activate pip install -r requirements.txt","title":"Home"},{"location":"#melampus-documenation","text":"Welcome! This the documentation for Melampus 0.0.3, the machine learning infrastracture package for SPHN-Imagine project Melampus consists of three modules: Preprocessor Feature Selector Classifier","title":"Melampus documenation"},{"location":"#installation","text":"For production mode: pip install -i https://test.pypi.org/simple/ quantimage-melampus Prerequisites commands for developers : source venv/bin/activate pip install -r requirements.txt","title":"Installation"},{"location":"classifier/","text":"Melampus classifier Melampus classifier gives users the option to train a supervised model on image datasets excracted by Kheops platform. It It provides as results metrics about its efficacy and accuracy. Especially, f1 score and confusion matrix are provided to the user into the classifier object. The initialization of a model contains two required input parameters. It also includes all the preprocessor steps as optional parameters (see Preprocessor ). e.g.: from melampus.classifier import MelampusClassifier mel_clf = MelampusClassifier(filename='synthetic_data/all.csv', algorithm_name='elastic_net', target_col='label', normalize=True, scaling=True, dim_red=(True, 5)) mel_clf.train() accuracy =mel_clf.metrics['accuracy'] precision = mel_clf.metrics['precision'] area_under_curve = mel_clf.metrics['area_under_curve'] recall = mel_clf.metrics['recall'] true_positives = mel_clf.metrics['true_pos'] true_negatives = mel_clf.metrics['true_neg'] false_positives = mel_clf.metrics['false_pos'] false_negatives = mel_clf.metrics['false_neg'] Initialization parameters The user must first initialize her classifier and then to use the train function to fit the data into the selected model. Required parameters filename : The name of the csv file that includes the data algorithm_name : The name of the desired method. Possible values: \"logistic_regression\": For Logistic Regression \"lasso_regression\": For logistic regression with the l1 penalty \"elastic_net\": For logistic regression with the elastic net penalty \"random_forest\": For Random Forest classifier, an embedded method of decision trees \"svm\": For a Support Vector Machine classifier. Optional parameters outcomes : the outcomes as a separated dataset in list format target_col : name of the target variable if included in the csv dataset scaling : Standarization of data dim_red : For high dimensional datasets. Reduce the amount of features into a new feature space. dimred[1] = number of dimentions in the new feature space normalize : Normalization with L2 data.","title":"Melampus classifier"},{"location":"classifier/#melampus-classifier","text":"Melampus classifier gives users the option to train a supervised model on image datasets excracted by Kheops platform. It It provides as results metrics about its efficacy and accuracy. Especially, f1 score and confusion matrix are provided to the user into the classifier object. The initialization of a model contains two required input parameters. It also includes all the preprocessor steps as optional parameters (see Preprocessor ). e.g.: from melampus.classifier import MelampusClassifier mel_clf = MelampusClassifier(filename='synthetic_data/all.csv', algorithm_name='elastic_net', target_col='label', normalize=True, scaling=True, dim_red=(True, 5)) mel_clf.train() accuracy =mel_clf.metrics['accuracy'] precision = mel_clf.metrics['precision'] area_under_curve = mel_clf.metrics['area_under_curve'] recall = mel_clf.metrics['recall'] true_positives = mel_clf.metrics['true_pos'] true_negatives = mel_clf.metrics['true_neg'] false_positives = mel_clf.metrics['false_pos'] false_negatives = mel_clf.metrics['false_neg']","title":"Melampus classifier"},{"location":"classifier/#initialization-parameters","text":"The user must first initialize her classifier and then to use the train function to fit the data into the selected model.","title":"Initialization parameters"},{"location":"classifier/#required-parameters","text":"filename : The name of the csv file that includes the data algorithm_name : The name of the desired method. Possible values: \"logistic_regression\": For Logistic Regression \"lasso_regression\": For logistic regression with the l1 penalty \"elastic_net\": For logistic regression with the elastic net penalty \"random_forest\": For Random Forest classifier, an embedded method of decision trees \"svm\": For a Support Vector Machine classifier.","title":"Required parameters"},{"location":"classifier/#optional-parameters","text":"outcomes : the outcomes as a separated dataset in list format target_col : name of the target variable if included in the csv dataset scaling : Standarization of data dim_red : For high dimensional datasets. Reduce the amount of features into a new feature space. dimred[1] = number of dimentions in the new feature space normalize : Normalization with L2 data.","title":"Optional parameters"},{"location":"feature_selector/","text":"Melampus Feature Selector Melampus Feature Selector provides three methods explained below. The initialization is same as the preprocessor's one (see Preprocessor for details) Methods variance_threshold : It removes all features whose variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features. :parameter (optionally) the threshold. default: 0.8 :returns data transformed (numpy array) drop_correlated_features : Drop all correlated features based on a specific metric and a correlation score. identify_correlated_features_with_target_variable : identify features correlated with the target variable The parameters are the same as at drop_correlated_features() It returns a a pandas dataframe All transormed data are returned in numpy array format from melampus.feature_selector import FeatureSelector fs = FeatureSelector(filename= '') x_tr = fs.variance_threshold() fs.rfe() x_tr = fs.drop_correlated_features(metric='pearson', score=0.9) selected_features = fs.identify_correlated_features_with_target_variable(score=0.95, metric= 'pearson', target_var= 'volT')","title":"Melampus Feature Selector"},{"location":"feature_selector/#melampus-feature-selector","text":"Melampus Feature Selector provides three methods explained below. The initialization is same as the preprocessor's one (see Preprocessor for details)","title":"Melampus Feature Selector"},{"location":"feature_selector/#methods","text":"variance_threshold : It removes all features whose variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features. :parameter (optionally) the threshold. default: 0.8 :returns data transformed (numpy array) drop_correlated_features : Drop all correlated features based on a specific metric and a correlation score. identify_correlated_features_with_target_variable : identify features correlated with the target variable The parameters are the same as at drop_correlated_features() It returns a a pandas dataframe All transormed data are returned in numpy array format from melampus.feature_selector import FeatureSelector fs = FeatureSelector(filename= '') x_tr = fs.variance_threshold() fs.rfe() x_tr = fs.drop_correlated_features(metric='pearson', score=0.9) selected_features = fs.identify_correlated_features_with_target_variable(score=0.95, metric= 'pearson', target_var= 'volT')","title":"Methods"},{"location":"preprocessor/","text":"Melampus Preprocessor Melampus Preprocessor accepts a csv file with the column names must be included . Also, the dataset must contains a separate column named exactly 'PatientID' with the samples ids. The name of the target variable can also be given as an optional parameter in case that the target variable is included in the csv file. Methods After the initialization, three function are provided for standarization, normalization and dimensionality reduction of the data: - standarize_data : standarization of data normalize_data : normalization of data with L2 norm dimensionality_reduction : For high dimensional datasets. It reduces the amount of features into a new feature space. Parameters: num_components : The dimensions of the new space You can access the transformed data on pre.data (format: numpy array) from melampus.preprocessor import Preprocessor pre = Preprocessor(filename='../synthetic_data/output_L0_GTVL.csv') pre.standarize_data() pre.normalize_data() pre.dimensionality_reduction(num_components=5) print('Processed data: {}'.format(pre.data)) Initialization parameters The user must first initialize her classifier and then to use the train function to fit the data into the selected model. Required parameters filename : The name of the csv file that includes the data Optional parameters target_col : name of the target variable if included in the csv dataset","title":"Melampus Preprocessor"},{"location":"preprocessor/#melampus-preprocessor","text":"Melampus Preprocessor accepts a csv file with the column names must be included . Also, the dataset must contains a separate column named exactly 'PatientID' with the samples ids. The name of the target variable can also be given as an optional parameter in case that the target variable is included in the csv file.","title":"Melampus Preprocessor"},{"location":"preprocessor/#methods","text":"After the initialization, three function are provided for standarization, normalization and dimensionality reduction of the data: - standarize_data : standarization of data normalize_data : normalization of data with L2 norm dimensionality_reduction : For high dimensional datasets. It reduces the amount of features into a new feature space. Parameters: num_components : The dimensions of the new space You can access the transformed data on pre.data (format: numpy array) from melampus.preprocessor import Preprocessor pre = Preprocessor(filename='../synthetic_data/output_L0_GTVL.csv') pre.standarize_data() pre.normalize_data() pre.dimensionality_reduction(num_components=5) print('Processed data: {}'.format(pre.data))","title":"Methods"},{"location":"preprocessor/#initialization-parameters","text":"The user must first initialize her classifier and then to use the train function to fit the data into the selected model.","title":"Initialization parameters"},{"location":"preprocessor/#required-parameters","text":"filename : The name of the csv file that includes the data","title":"Required parameters"},{"location":"preprocessor/#optional-parameters","text":"target_col : name of the target variable if included in the csv dataset","title":"Optional parameters"}]}